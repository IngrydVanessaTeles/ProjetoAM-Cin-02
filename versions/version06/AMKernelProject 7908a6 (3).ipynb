{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import MultinomialNB,BernoulliNB\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn import linear_model\nimport nltk\nimport random\nimport re\nimport nltk.corpus\nimport sklearn\nimport numpy\nimport csv\nfrom sklearn import metrics\nimport sys\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=True) #se não ignorar os stopwords, não vai remover eles depois\n\n#inicializando bag de stopwords\nstopwords = nltk.corpus.stopwords.words('english')\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['cleandata', 'jigsaw-unintended-bias-in-toxicity-classification']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"base = []\ncsvFile = \"../input/cleandata/cleanDataTrain.csv\"\n\nbaseTrain = []\nlabels = []\nallWords = []\nwith open(csvFile) as csvfile:\n    import codecs\n    ifile = open(csvFile, \"rb\")\n    read = csv.reader(codecs.iterdecode(ifile, 'utf-8'))\n\n    for row in read:\n        try:\n            temp1 = row[1]\n            allWords.append(temp1)\n            baseTrain.append(temp1)\n            temp2 = float(row[0])\n            labels.append(temp2)\n        except IndexError:\n            pass","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseTest = []\ncsvFile = \"../input/cleandata/cleanDataTest.csv\"\nids = []\nwith open(csvFile) as csvfile:\n    import codecs\n    ifile = open(csvFile, \"rb\")\n    read = csv.reader(codecs.iterdecode(ifile, 'utf-8'))\n\n    for row in read:\n        try:\n            # Se for pegar o valor float\n            temp2 = row[0]\n            temp1 = row[1]\n            if(temp2!=\"id\"):\n                baseTest.append(temp1)\n                ids.append(temp2)\n        except IndexError:\n            pass","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = baseTrain\ny = labels","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_min = []\ny_min = []\nfor u in range(len(y)):\n    if(y[u]!=0):\n        x_min.append(x[u])\n        y_min.append(y[u])","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#vectorizer_TF = CountVectorizer()\n#X_TFIDF = vectorizer_TF.fit_transform(x_min)\n#X_TestTFIDF = vectorizer_TF.transform(baseTest)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_TFIDF = TfidfVectorizer()\nX_TFIDF = vectorizer_TFIDF.fit_transform(x_min)\nX_TestTFIDF = vectorizer_TFIDF.transform(baseTest)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y_min, test_size=0.10, random_state=42)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nAcoo_trainX = X_train.tocoo()\nApt_trainX = torch.sparse.LongTensor(torch.LongTensor([Acoo_trainX.row.tolist(), Acoo_trainX.col.tolist()]),\n                              torch.LongTensor(Acoo_trainX.data.astype(numpy.int32)))\nAcoo_testX = X_test.tocoo()\nApt_testX = torch.sparse.LongTensor(torch.LongTensor([Acoo_testX.row.tolist(), Acoo_testX.col.tolist()]),\n                              torch.LongTensor(Acoo_testX.data.astype(numpy.int32)))\nAcoo_trainY = numpy.asarray(y_train)\nApt_trainY = torch.from_numpy(Acoo_trainY)\n\nAcoo_testY = numpy.asarray(y_test)\nApt_testY = torch.from_numpy(Acoo_testY)\n\n# create Tensor datasets\ntrain_data = TensorDataset(Apt_trainX, Apt_trainY)\ntest_data = TensorDataset(Apt_testX, Apt_testY)\n\n# dataloaders\nbatch_size = 50\n\n# shuffling and batching data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":18,"outputs":[{"output_type":"stream","text":"Training on GPU.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SentimentCNN(nn.Module):\n    \"\"\"\n    The embedding layer + CNN model that will be used to perform sentiment analysis.\n    \"\"\"\n\n    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentCNN, self).__init__()\n\n        # set class vars\n        self.num_filters = num_filters\n        self.embedding_dim = embedding_dim\n        \n        # 1. embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # set weights to pre-trained\n        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n        # (optional) freeze embedding weights\n        if freeze_embeddings:\n            self.embedding.requires_grad = False\n        \n        # 2. convolutional layers\n        self.convs_1d = nn.ModuleList([\n            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=k-2) \n            for k in kernel_sizes])\n        \n        # 3. final, fully-connected layer for classification\n        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n        \n        # 4. dropout and sigmoid layers\n        self.dropout = nn.Dropout(drop_prob)\n        self.sig = nn.Sigmoid()\n        \n    \n    def conv_and_pool(self, x, conv):\n        \"\"\"\n        Convolutional + max pooling layer\n        \"\"\"\n        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n        # conv_seq_length will be ~ 200\n        x = F.relu(conv(x)).squeeze(3)\n        \n        # 1D pool over conv_seq_length\n        # squeeze to get size: (batch_size, num_filters)\n        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x_max\n\n    def forward(self, x):\n        \"\"\"\n        Defines how a batch of inputs, x, passes through the model layers.\n        Returns a single, sigmoid-activated class score as output.\n        \"\"\"\n        # embedded vectors\n        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n        embeds = embeds.unsqueeze(1)\n        \n        # get output of each conv-pool layer\n        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n        \n        # concatenate results and add dropout\n        x = torch.cat(conv_results, 1)\n        x = self.dropout(x)\n        \n        # final logit\n        logit = self.fc(x) \n        \n        # sigmoid-activated --> a class score\n        return self.sig(logit)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_TFIDF.shape[1]","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"109767"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\n\nvocab_size = X_TFIDF.shape[1]\nembed_lookup = vectorizer_TFIDF\noutput_size = 1 # binary class (1 or 0)\nembedding_dim = vocab_size # 300-dim vectors\nnum_filters = 100\nkernel_sizes = [3, 4, 5]\n\nnet = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n                   num_filters, kernel_sizes)\n\nprint(net)","execution_count":29,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"[enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-7630ea4913d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n\u001b[0;32m---> 11\u001b[0;31m                    num_filters, kernel_sizes)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-6531b4c9925c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_model, vocab_size, output_size, embedding_dim, num_filters, kernel_sizes, freeze_embeddings, drop_prob)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 1. embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# set weights to pre-trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# all vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n#clf = svm.SVR()\n#clf = linear_model.LinearRegression()\n#clf = DecisionTreeRegressor(max_depth=10)\nclf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\n#clf = GaussianNB()\n\n#Classificação com probabilidade\ny_train_class = []\nfor t in range(len(y_train)):\n    if(y_train[t]<0.5):\n        y_train_class.append(0)\n    else:\n        y_train_class.append(1)\n\nclf.fit(X_train,y_train_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ny_pred = clf.predict(X_test) \ny_pred_prob = clf.predict_proba(X_test) \nprint(y_pred_prob)\nprint(X_test.shape)\n\ny_pred_class = y_pred\ny_test_class = []\ny_pred_prob_tox = []\n\n\nfor t in range(len(y_test)):\n    if(y_test[t]<0.5):\n        y_test_class.append(0)\n    else:\n        y_test_class.append(1)\n    y_pred_prob_tox.append(y_pred_prob[t][0])\n\nmse = metrics.mean_squared_error(y_test, y_pred_prob_tox)\nauc = metrics.roc_auc_score(y_test_class, y_pred_class)\nacc = metrics.accuracy_score(y_test_class, y_pred_class)\nprec = metrics.precision_score(y_test_class, y_pred_class)\nrec = metrics.recall_score(y_test_class, y_pred_class)\n\nprint(auc)\nprint(acc)\nprint(prec)\nprint(rec)\nprint(mse)\n\nwith open('results_validation.csv', mode='w') as employee_file:\n    writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    writer.writerow([\"Logistic Regression\"])\n    writer.writerow([\"mse: \", mse])\n    writer.writerow([\"auc: \", auc])\n    writer.writerow([\"accuracy: \", acc])\n    writer.writerow([\"precision: \", prec])\n    writer.writerow([\"recall: \", rec])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn import metrics\n# y_pred = clf.predict(X_test) \n\n# y_pred_class = []\n# y_test_class = []\n\n# for t in range(len(y_test)):\n#     if(y_test[t]<0.5):\n#         y_test_class.append(0)\n#     else:\n#         y_test_class.append(1)\n#     if(y_pred[t]<0.5):\n#         y_pred_class.append(0)\n#     else:\n#         y_pred_class.append(1)\n\n# mse = metrics.mean_squared_error(y_test, y_pred)\n# auc = metrics.roc_auc_score(y_test_class, y_pred_class)\n# acc = metrics.accuracy_score(y_test_class, y_pred_class)\n# prec = metrics.precision_score(y_test_class, y_pred_class)\n# rec = metrics.recall_score(y_test_class, y_pred_class)\n\n# print(auc)\n# print(mse)\n# print(acc)\n# print(prec)\n# print(rec)\n\n# with open('results_validation.csv', mode='w') as employee_file:\n#     writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n#     writer.writerow([\"Linear Regression\"])\n#     writer.writerow([\"mse: \", mse])\n#     writer.writerow([\"auc: \", auc])\n#     writer.writerow([\"accuracy: \", acc])\n#     writer.writerow([\"precision: \", prec])\n#     writer.writerow([\"recall: \", rec])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test = clf.predict_proba(X_TestTFIDF)\nwith open('submission.csv', mode='w') as employee_file:\n    \n    writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    writer.writerow([\"id\", \"prediction\"])\n    for id in range(len(ids)):\n        print(ids[id])\n        print(y_pred_test[id][1])\n        writer.writerow([ids[id], y_pred_test[id][1]])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}